# Feature 09: Real LLM Token Streaming

## Status: Proposed

## Summary

Replace the simulated streaming implementation in the `/v1/chat/completions` endpoint with true LLM token-by-token streaming. Currently, the backend generates the full response, then chunks it into 10-character pieces with artificial delays. This feature wires real streaming from the LLM provider (Ollama, OpenAI, etc.) through the LangGraph workflow and out to the client via SSE.

## Problem Statement

The current streaming implementation creates a poor user experience:
- **High Time-to-First-Token (TTFT):** Users see "Thinking..." for the entire LLM generation time before any content appears, because the full response must complete before chunking begins.
- **Artificial chunking:** The 10-char chunks with 0.01s delays create a robotic feel rather than the natural token-by-token flow users expect from modern AI interfaces.
- **No partial rendering:** The UI cannot render markdown progressively as the model thinks, which is especially impactful for long responses with code blocks or tables.

## Proposed Solution

### Backend Changes

1. **LLM streaming integration** (`src/llm.py` or provider-specific adapters):
   - Use `model.astream()` from LangChain instead of `model.ainvoke()` for streaming requests
   - Yield `AIMessageChunk` tokens as they arrive from the provider
   - Support Ollama (native streaming) and OpenAI API (SSE streaming)

2. **Graph workflow streaming** (`src/graph/workflows.py`):
   - Use LangGraph's `astream_events()` or `astream()` to propagate streaming tokens through the graph
   - Filter events to only yield `on_chat_model_stream` events (actual LLM output tokens)
   - Handle tool calls gracefully (don't stream tool call JSON, only stream final response text)

3. **SSE endpoint update** (`src/api/routes/openai_compat.py`):
   - Replace the simulate-streaming loop with a real async generator that yields tokens from the graph
   - Maintain OpenAI-compatible `data: {...}\n\n` SSE format
   - Properly handle `finish_reason` transitions (null during streaming, "stop" on final chunk)
   - Handle errors mid-stream gracefully (send error chunk then `[DONE]`)

4. **Tool call handling:**
   - When the LLM makes tool calls (MCP, entity lookups, etc.), pause token streaming
   - Optionally send a status event: `data: {"type": "status", "content": "Checking entities..."}`
   - Resume token streaming when the LLM generates its final response after tool execution

### Frontend Changes (Minimal)

The frontend (`ui/src/api/client.ts`) already correctly parses SSE streams and handles incremental content. No changes needed for basic streaming. Optional enhancements:
- Parse status events to show "Checking entities..." / "Running analysis..." during tool calls
- Debounce markdown re-rendering during fast token streams (every 100ms or every 5 tokens)

## Architecture

```
User Input
    |
    v
POST /v1/chat/completions (stream=true)
    |
    v
LangGraph Workflow (astream_events)
    |
    +---> LLM Token Stream (astream on ChatModel)
    |         |
    |         +---> SSE chunk: {"delta": {"content": "token"}}
    |
    +---> Tool Call (pause streaming)
    |         |
    |         +---> SSE chunk: {"type": "status", "content": "..."}
    |         +---> Execute tool, feed result back to LLM
    |         +---> Resume LLM streaming
    |
    +---> Final token
              |
              +---> SSE chunk: {"finish_reason": "stop"}
              +---> SSE: data: [DONE]
```

## Acceptance Criteria

- [ ] Time-to-first-token < 500ms for simple queries (excluding LLM cold start)
- [ ] Tokens appear in the UI as they're generated by the LLM
- [ ] No regression in tool call functionality (tools still execute correctly)
- [ ] Error handling: mid-stream errors are reported to the client gracefully
- [ ] Conversation history is still saved correctly after streaming completes
- [ ] MLflow tracing still captures full response and token counts
- [ ] Works with both Ollama and OpenAI-compatible providers

## Dependencies

- LangChain's `astream()` / `astream_events()` API
- LangGraph streaming support
- Provider-specific streaming support (Ollama, OpenAI)

## Estimated Effort

**Medium-High** - Primarily backend work touching the graph execution layer, LLM integration, and SSE endpoint. Frontend changes are minimal since the streaming client already handles SSE correctly.

## References

- [LangGraph Streaming Guide](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/)
- [LangChain astream_events](https://python.langchain.com/docs/how_to/streaming/)
- [OpenAI Streaming API](https://platform.openai.com/docs/api-reference/streaming)
