# =============================================================================
# Project Aether - Local Development Stack
# =============================================================================
# Usage:
#   podman-compose up -d          # Start all services
#   podman-compose logs -f        # View logs
#   podman-compose down           # Stop and remove
#   podman-compose down -v        # Stop and remove with volumes

services:
  # ===========================================================================
  # PostgreSQL Database (Constitution: Principle IV - State)
  # ===========================================================================
  postgres:
    image: docker.io/library/postgres:16-alpine
    container_name: aether-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: aether
      POSTGRES_PASSWORD: aether
      POSTGRES_DB: aether
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./infrastructure/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U aether -d aether"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ===========================================================================
  # MLflow Tracking Server (Constitution: Principle III - Observability)
  # ===========================================================================
  # NOTE: Using SQLite backend because MLflow v3.9.0 has a bug with PostgreSQL
  # that causes trace ingestion to fail with 500 errors. Runs/experiments work
  # fine with PostgreSQL, but traces require SQLite until MLflow fixes the bug.
  # To use PostgreSQL (without traces), set MLFLOW_USE_POSTGRES=true
  mlflow:
    image: ghcr.io/mlflow/mlflow:v3.9.0
    container_name: aether-mlflow
    restart: unless-stopped
    environment:
      MLFLOW_DEFAULT_ARTIFACT_ROOT: /mlflow/artifacts
    ports:
      - "${MLFLOW_PORT:-5002}:5000"
    volumes:
      - mlflow_data:/mlflow/data
      - mlflow_artifacts:/mlflow/artifacts
    command: >
      mlflow server
      --host 0.0.0.0
      --port 5000
      --backend-store-uri sqlite:////mlflow/data/mlflow.db
      --default-artifact-root /mlflow/artifacts
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ===========================================================================
  # Redis Cache (Optional - for entity caching)
  # ===========================================================================
  redis:
    image: docker.io/library/redis:7-alpine
    container_name: aether-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ===========================================================================
  # Aether Application (uncomment for containerized development)
  # ===========================================================================
  # app:
  #   build:
  #     context: ../..
  #     dockerfile: infrastructure/podman/Containerfile
  #   container_name: aether-app
  #   restart: unless-stopped
  #   env_file:
  #     - ../../.env
  #   environment:
  #     DATABASE_URL: postgresql+asyncpg://aether:aether@postgres:5432/aether
  #     MLFLOW_TRACKING_URI: http://mlflow:5000
  #   ports:
  #     - "8000:8000"
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #     mlflow:
  #       condition: service_healthy
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 3

  # ===========================================================================
  # Open WebUI - Chat Interface
  # ===========================================================================
  # Provides a beautiful chat UI that connects to Aether's OpenAI-compatible API.
  # Access at http://localhost:3000 after starting.
  #
  # Configuration:
  #   - OPENAI_API_BASE_URL points to Aether's /v1 endpoint
  #   - OPENAI_API_KEY is required but not validated (use any value)
  #   - WEBUI_AUTH=False disables auth for local development
  #
  # Note: Aether API must be running on host for Open WebUI to connect.
  # Use `host.containers.internal` for container -> host networking.
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: aether-webui
    restart: unless-stopped
    environment:
      # Point to Aether's OpenAI-compatible API
      # Note: host.containers.internal resolves to host machine in Podman
      OPENAI_API_BASE_URL: ${AETHER_API_URL:-http://host.containers.internal:8000/api}/v1
      OPENAI_API_KEY: ${OPENAI_API_KEY:-sk-aether-local}
      # Disable authentication for local development
      WEBUI_AUTH: "False"
      # Disable telemetry
      SCARF_NO_ANALYTICS: "true"
      DO_NOT_TRACK: "true"
      ANONYMIZED_TELEMETRY: "false"
      # Default model
      DEFAULT_MODELS: "architect"
    ports:
      - "${WEBUI_PORT:-3000}:8080"
    volumes:
      - open_webui_data:/app/backend/data
    extra_hosts:
      # Required for container to reach host on Linux
      - "host.containers.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  postgres_data:
    name: aether-postgres-data
  mlflow_data:
    name: aether-mlflow-data
  mlflow_artifacts:
    name: aether-mlflow-artifacts
  redis_data:
    name: aether-redis-data
  open_webui_data:
    name: aether-webui-data

networks:
  default:
    name: aether-network
